{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borra logs anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree('logs', ignore_errors=True)\n",
    "shutil.rmtree('logs2', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importa librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1\n",
    "\n",
    "Dataset de números del 0 al 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/camilo/anaconda3/envs/vision/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/camilo/anaconda3/envs/vision/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/camilo/anaconda3/envs/vision/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/camilo/anaconda3/envs/vision/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/camilo/anaconda3/envs/vision/lib/python3.6/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desordena el dataset y lo divide en entrenamiento, validación y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, Y_train_ = shuffle(mnist.train.images, mnist.train.labels) \n",
    "X_train_orig, X_val_orig, Y_train, Y_val = train_test_split(X_train_, Y_train_, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train_orig, (-1, 28, 28))\n",
    "X_val = np.reshape(X_val_orig, (-1, 28, 28))\n",
    "X_test = np.reshape(mnist.test.images, (-1, 28, 28))\n",
    "\n",
    "X_train = np.pad(X_train , ((0,0),(2,2),(2,2)), 'constant')\n",
    "X_val = np.pad(X_val , ((0,0),(2,2),(2,2)), 'constant')\n",
    "X_test = np.pad(X_test , ((0,0),(2,2),(2,2)), 'constant')\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val = X_val[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (44000, 32, 32, 1), Y_train: (44000, 10)\n",
      "X_val: (11000, 32, 32, 1), Y_val: (11000, 10)\n",
      "X_test: (10000, 32, 32, 1), Y_test: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train: {X_train.shape}, Y_train: {Y_train.shape}')\n",
    "print(f'X_val: {X_val.shape}, Y_val: {Y_val.shape}')\n",
    "print(f'X_test: {X_test.shape}, Y_test: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc44c1036a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[12][:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capas ocultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/camilo/anaconda3/envs/vision/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/camilo/anaconda3/envs/vision/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "  \n",
    "# Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "mu = 0\n",
    "sigma = 0.1    \n",
    "\n",
    "weights = {\n",
    "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "    'conv1': tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma)),\n",
    "    'conv2': tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma)),\n",
    "    'fl1': tf.Variable(tf.truncated_normal(shape=(5 * 5 * 16, 120), mean = mu, stddev = sigma)),\n",
    "    'fl2': tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma)),\n",
    "    'out': tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean = mu, stddev = sigma))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    # The shape of the filter bias is (output_depth,)\n",
    "    'conv1': tf.Variable(tf.zeros(6)),\n",
    "    'conv2': tf.Variable(tf.zeros(16)),\n",
    "    'fl1': tf.Variable(tf.zeros(120)),\n",
    "    'fl2': tf.Variable(tf.zeros(84)),\n",
    "    'out': tf.Variable(tf.zeros(n_classes))\n",
    "}\n",
    "\n",
    "# Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "conv1 = tf.nn.conv2d(x, weights['conv1'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "conv1 = tf.nn.bias_add(conv1, biases['conv1'])\n",
    "# Activation.\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "# Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "conv1 = tf.nn.avg_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# Layer 2: Convolutional. Output = 10x10x16.\n",
    "conv2 = tf.nn.conv2d(conv1, weights['conv2'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "conv2 = tf.nn.bias_add(conv2, biases['conv2'])\n",
    "# Activation.\n",
    "conv2 = tf.nn.relu(conv2)\n",
    "# Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "conv2 = tf.nn.avg_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# Flatten. Input = 5x5x16. Output = 400.\n",
    "fl0 = flatten(conv2)\n",
    "\n",
    "# Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "fl1 = tf.add(tf.matmul(fl0, weights['fl1']), biases['fl1'])\n",
    "# Activation.\n",
    "fl1 = tf.nn.relu(fl1)\n",
    "\n",
    "# Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "fl2 = tf.add(tf.matmul(fl1, weights['fl2']), biases['fl2'])\n",
    "# Activation.\n",
    "fl2 = tf.nn.relu(fl2)\n",
    "\n",
    "# Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "logits = tf.add(tf.matmul(fl2, weights['out']), biases['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-fc6d1f807428>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas para Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [\n",
    "    tf.summary.histogram(\"weights/conv1\", weights['conv1']),\n",
    "    tf.summary.histogram(\"weights/conv2\", weights['conv2']),\n",
    "    tf.summary.histogram(\"weights/fl1\", weights['fl1']),\n",
    "    tf.summary.histogram(\"weights/fl2\", weights['fl2']),\n",
    "    tf.summary.histogram(\"weights/out\", weights['out']),\n",
    "    tf.summary.histogram(\"biases/conv1\", biases['conv1']),\n",
    "    tf.summary.histogram(\"biases/conv2\", biases['conv2']),\n",
    "    tf.summary.histogram(\"biases/fl1\", biases['fl1']),\n",
    "    tf.summary.histogram(\"biases/fl2\", biases['fl2']),\n",
    "    tf.summary.histogram(\"biases/conv1\", biases['out']),\n",
    "    tf.summary.scalar('loss', loss_operation),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo de precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy_operation)\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy, summ = sess.run([accuracy_operation, accuracy_summary], feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        \n",
    "    return total_accuracy / num_examples, summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del dataset de 0 a 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las variables se modifican en la optimización, para el primer entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_opt = [ weights['conv1'], weights['conv2'], weights['fl1'], weights['fl2'], weights['out'], \n",
    "                biases['conv1'], biases['conv2'], biases['fl1'], biases['fl2'], biases['out'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_operation = optimizer.minimize(loss_operation, var_list=vars_to_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LeNet...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.956\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.963\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.975\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.977\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.981\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.982\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.980\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.984\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.984\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.984\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    writer = tf.summary.FileWriter(\"./logs\", session.graph)\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    print(\"Training LeNet...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train_epoch, Y_train_epoch = shuffle(X_train, Y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train_epoch[offset:end], Y_train_epoch[offset:end]\n",
    "            summs = session.run([training_operation]+summaries, feed_dict={x: batch_x, y: batch_y})\n",
    "            summs.pop(0)\n",
    "            for summ in summs:\n",
    "                writer.add_summary(summ, global_step=step)\n",
    "            step += 1\n",
    "            \n",
    "        validation_accuracy, validation_summary = evaluate(X_val, Y_val)\n",
    "        writer.add_summary(validation_summary, global_step=step)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    saver.save(session, './models/lenet/lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del dataset de 0 a 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./models/lenet'))\n",
    "\n",
    "    test_accuracy, _ = evaluate(X_test, Y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset de letras de la A a la J\n",
    "EMNIST es MNIST extendido, tiene letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emnist import extract_training_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera vez, se demora descargando el dataset (aproximadamente 536MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig, Y_orig_idx = extract_training_samples('letters')\n",
    "A_J_ixd = Y_orig_idx<=10\n",
    "\n",
    "X_orig = X_orig[A_J_ixd]\n",
    "X_orig = np.pad(X_orig , ((0,0),(2,2),(2,2)), 'constant')\n",
    "X_orig = X_orig[..., np.newaxis]\n",
    "X_orig = X_orig / 255.0\n",
    "\n",
    "Y_orig_idx = Y_orig_idx[A_J_ixd] - 1\n",
    "Y_orig = np.zeros((Y_orig_idx.size, Y_orig_idx.max()+1))\n",
    "Y_orig[np.arange(Y_orig_idx.size),Y_orig_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_orig: {X_orig.shape}, Y_orig: {Y_orig.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 582\n",
    "plt.imshow(X_orig[idx][:,:,0], cmap='gray')\n",
    "print(f'class: {Y_orig_idx[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig, Y_orig = shuffle(X_orig, Y_orig) \n",
    "X_orig_, X_test, Y_orig_, Y_test = train_test_split(X_orig, Y_orig, test_size=0.2)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_orig_, Y_orig_, test_size=0.2)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, Y_train: {Y_train.shape}')\n",
    "print(f'X_val: {X_val.shape}, Y_val: {Y_val.shape}')\n",
    "print(f'X_test: {X_test.shape}, Y_test: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probando dataset de letras con entrenamiento de números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./models/lenet'))\n",
    "\n",
    "    test_accuracy, _ = evaluate(X_test, Y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizador para entrenamiento de letras\n",
    "\n",
    "No se tocan las capas convolucionales, únicamente se modifican las fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_opt_2 = [ weights['fl1'], weights['fl2'], weights['out'], \n",
    "                 biases['fl1'], biases['fl2'], biases['out'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer_2 = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_operation_2 = optimizer_2.minimize(loss_operation, var_list=vars_to_opt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento para letras de A a J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./models/lenet'))\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"./logs2\", session.graph)\n",
    "    \n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    print(\"Training LeNet 2...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train_epoch, Y_train_epoch = shuffle(X_train, Y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train_epoch[offset:end], Y_train_epoch[offset:end]\n",
    "            summs = session.run([training_operation_2]+summaries, feed_dict={x: batch_x, y: batch_y})\n",
    "            summs.pop(0)\n",
    "            for summ in summs:\n",
    "                writer.add_summary(summ, global_step=step)\n",
    "            step += 1\n",
    "            \n",
    "        validation_accuracy, validation_summary = evaluate(X_val, Y_val)\n",
    "        writer.add_summary(validation_summary, global_step=step)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    saver.save(session, './models/lenet2/lenet2')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba para letras de A a J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./models/lenet2'))\n",
    "\n",
    "    test_accuracy, _ = evaluate(X_test, Y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
