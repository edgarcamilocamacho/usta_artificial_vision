{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borra logs anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree('logs', ignore_errors=True)\n",
    "shutil.rmtree('logs2', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importa librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1\n",
    "\n",
    "Dataset de números del 0 al 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desordena el dataset y lo divide en entrenamiento, validación y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, Y_train_ = shuffle(mnist.train.images, mnist.train.labels) \n",
    "X_train_orig, X_val_orig, Y_train, Y_val = train_test_split(X_train_, Y_train_, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train_orig, (-1, 28, 28))\n",
    "X_val = np.reshape(X_val_orig, (-1, 28, 28))\n",
    "X_test = np.reshape(mnist.test.images, (-1, 28, 28))\n",
    "\n",
    "X_train = np.pad(X_train , ((0,0),(2,2),(2,2)), 'constant')\n",
    "X_val = np.pad(X_val , ((0,0),(2,2),(2,2)), 'constant')\n",
    "X_test = np.pad(X_test , ((0,0),(2,2),(2,2)), 'constant')\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_val = X_val[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train: {X_train.shape}, Y_train: {Y_train.shape}')\n",
    "print(f'X_val: {X_val.shape}, Y_val: {Y_val.shape}')\n",
    "print(f'X_test: {X_test.shape}, Y_test: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[15][:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capas ocultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "  \n",
    "# Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "mu = 0\n",
    "sigma = 0.1    \n",
    "\n",
    "weights = {\n",
    "    # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "    'conv1': tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma)),\n",
    "    'conv2': tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma)),\n",
    "    'fl1': tf.Variable(tf.truncated_normal(shape=(5 * 5 * 16, 120), mean = mu, stddev = sigma)),\n",
    "    'fl2': tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma)),\n",
    "    'out': tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean = mu, stddev = sigma))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    # The shape of the filter bias is (output_depth,)\n",
    "    'conv1': tf.Variable(tf.zeros(6)),\n",
    "    'conv2': tf.Variable(tf.zeros(16)),\n",
    "    'fl1': tf.Variable(tf.zeros(120)),\n",
    "    'fl2': tf.Variable(tf.zeros(84)),\n",
    "    'out': tf.Variable(tf.zeros(n_classes))\n",
    "}\n",
    "\n",
    "# Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "conv1 = tf.nn.conv2d(x, weights['conv1'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "conv1 = tf.nn.bias_add(conv1, biases['conv1'])\n",
    "# Activation.\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "# Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "conv1 = tf.nn.avg_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# Layer 2: Convolutional. Output = 10x10x16.\n",
    "conv2 = tf.nn.conv2d(conv1, weights['conv2'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "conv2 = tf.nn.bias_add(conv2, biases['conv2'])\n",
    "# Activation.\n",
    "conv2 = tf.nn.relu(conv2)\n",
    "# Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "conv2 = tf.nn.avg_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# Flatten. Input = 5x5x16. Output = 400.\n",
    "fl0 = flatten(conv2)\n",
    "\n",
    "# Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "fl1 = tf.add(tf.matmul(fl0, weights['fl1']), biases['fl1'])\n",
    "# Activation.\n",
    "fl1 = tf.nn.relu(fl1)\n",
    "\n",
    "# Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "fl2 = tf.add(tf.matmul(fl1, weights['fl2']), biases['fl2'])\n",
    "# Activation.\n",
    "fl2 = tf.nn.relu(fl2)\n",
    "\n",
    "# Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "logits = tf.add(tf.matmul(fl2, weights['out']), biases['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas para Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [\n",
    "    tf.summary.histogram(\"weights/conv1\", weights['conv1']),\n",
    "    tf.summary.histogram(\"weights/conv2\", weights['conv2']),\n",
    "    tf.summary.histogram(\"weights/fl1\", weights['fl1']),\n",
    "    tf.summary.histogram(\"weights/fl2\", weights['fl2']),\n",
    "    tf.summary.histogram(\"weights/out\", weights['out']),\n",
    "    tf.summary.histogram(\"biases/conv1\", biases['conv1']),\n",
    "    tf.summary.histogram(\"biases/conv2\", biases['conv2']),\n",
    "    tf.summary.histogram(\"biases/fl1\", biases['fl1']),\n",
    "    tf.summary.histogram(\"biases/fl2\", biases['fl2']),\n",
    "    tf.summary.histogram(\"biases/conv1\", biases['out']),\n",
    "    tf.summary.scalar('loss', loss_operation),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo de precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy_operation)\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy, summ = sess.run([accuracy_operation, accuracy_summary], feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        \n",
    "    return total_accuracy / num_examples, summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del dataset de 0 a 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las variables se modifican en la optimización, para el primer entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_opt = [ weights['conv1'], weights['conv2'], weights['fl1'], weights['fl2'], weights['out'], \n",
    "                biases['conv1'], biases['conv2'], biases['fl1'], biases['fl2'], biases['out'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_operation = optimizer.minimize(loss_operation, var_list=vars_to_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    writer = tf.summary.FileWriter(\"./logs\", session.graph)\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    print(\"Training LeNet...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train_epoch, Y_train_epoch = shuffle(X_train, Y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train_epoch[offset:end], Y_train_epoch[offset:end]\n",
    "            summs = session.run([training_operation]+summaries, feed_dict={x: batch_x, y: batch_y})\n",
    "            summs.pop(0)\n",
    "            for summ in summs:\n",
    "                writer.add_summary(summ, global_step=step)\n",
    "            step += 1\n",
    "            \n",
    "        validation_accuracy, validation_summary = evaluate(X_val, Y_val)\n",
    "        writer.add_summary(validation_summary, global_step=step)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    saver.save(session, './models/lenet/lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del dataset de 0 a 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./models/lenet'))\n",
    "\n",
    "    test_accuracy, _ = evaluate(X_test, Y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset de letras de la A a la J\n",
    "EMNIST es MNIST extendido, tiene letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emnist import extract_training_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera vez, se demora descargando el dataset (aproximadamente 536MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig, Y_orig_idx = extract_training_samples('letters')\n",
    "A_J_ixd = Y_orig_idx<=10\n",
    "\n",
    "X_orig = X_orig[A_J_ixd]\n",
    "X_orig = np.pad(X_orig , ((0,0),(2,2),(2,2)), 'constant')\n",
    "X_orig = X_orig[..., np.newaxis]\n",
    "X_orig = X_orig / 255.0\n",
    "\n",
    "Y_orig_idx = Y_orig_idx[A_J_ixd] - 1\n",
    "Y_orig = np.zeros((Y_orig_idx.size, Y_orig_idx.max()+1))\n",
    "Y_orig[np.arange(Y_orig_idx.size),Y_orig_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_orig: {X_orig.shape}, Y_orig: {Y_orig.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 582\n",
    "plt.imshow(X_orig[idx][:,:,0], cmap='gray')\n",
    "print(f'class: {Y_orig_idx[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig, Y_orig = shuffle(X_orig, Y_orig) \n",
    "X_orig_, X_test, Y_orig_, Y_test = train_test_split(X_orig, Y_orig, test_size=0.2)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_orig_, Y_orig_, test_size=0.2)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, Y_train: {Y_train.shape}')\n",
    "print(f'X_val: {X_val.shape}, Y_val: {Y_val.shape}')\n",
    "print(f'X_test: {X_test.shape}, Y_test: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probando dataset de letras con entrenamiento de números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./models/lenet'))\n",
    "\n",
    "    test_accuracy, _ = evaluate(X_test, Y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizador para entrenamiento de letras\n",
    "\n",
    "No se tocan las capas convolucionales, únicamente se modifican las fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_opt_2 = [ weights['fl1'], weights['fl2'], weights['out'], \n",
    "                 biases['fl1'], biases['fl2'], biases['out'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer_2 = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_operation_2 = optimizer_2.minimize(loss_operation, var_list=vars_to_opt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento para letras de A a J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./models/lenet'))\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"./logs2\", session.graph)\n",
    "    \n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    print(\"Training LeNet 2...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train_epoch, Y_train_epoch = shuffle(X_train, Y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train_epoch[offset:end], Y_train_epoch[offset:end]\n",
    "            summs = session.run([training_operation_2]+summaries, feed_dict={x: batch_x, y: batch_y})\n",
    "            summs.pop(0)\n",
    "            for summ in summs:\n",
    "                writer.add_summary(summ, global_step=step)\n",
    "            step += 1\n",
    "            \n",
    "        validation_accuracy, validation_summary = evaluate(X_val, Y_val)\n",
    "        writer.add_summary(validation_summary, global_step=step)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    saver.save(session, './models/lenet2/lenet2')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba para letras de A a J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./models/lenet2'))\n",
    "\n",
    "    test_accuracy, _ = evaluate(X_test, Y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
